Since we need to know how to take each step, i.e., get the action, we use the Q-learning method.
\subsubsection{Q value}
The Q value is $Q_{k}(s,a)$
where s denotes the current state of the game and a denotes the current action.
We use the board to represent the current state of the game. Considering that the 
starting point and the landing point of this action will be different, we define the 
action as a tuple, which records which piece is picked and where this piece lands.
\subsubsection{Reward function}

\subsubsection{Training design}
Considering the training time, we train our Qlearning agent as a red side with Random agent on the black side. 
After Qlearning agent makes a move, we record the current state of the board and its 
actions, and from these two we calculate the reward that the Qlearning agent gets in this move 
as r1. Next, after the random agent makes its next move, we get the reward 
that the Random agent gets in this move as r2. Then, at the end of this round, the actual 
reward that Red gets is (r1-r2). So the sample estimate is
\begin{align*}
    sample=(r1-r2)+\gamma max_{a'}Q(s',a')
\end{align*}
Then we update the q-value of $Q(s,a)$
\begin{align*}
    Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha [sample]
\end{align*}
Since we need to explore as many different states as possible in the early stages of training, 
we introduced the $\epsilon$-greedy function. Setting epsilon to 0.5 gives our agent a 50 percent chance of 
picking the optimal policy based on $argmax_{a}Q(s,a)$ and a 50 percent chance of taking the policy randomly.

\subsubsection{Training}
Due to memory limitations, the training limit is 4000 epochs. The gamma is set to 0.8 and the 
learning rate is set to 0.8 for the first 1000 epochs, 0.5 for 1000 to 2000 epochs, 0.2 for 2000 
to 3000 epochs, and 0.1 for 3000 to 4000 epochs.
We write the explored qvalue to a txt file in the form of json. All values of the current Q(s,a) are 
recorded once every 100 epochs.